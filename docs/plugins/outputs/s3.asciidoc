[[plugins-outputs-s3]]
=== s3

* Version: 4.0.6
* Released on: February 23, 2017
* https://github.com/logstash-plugins/logstash-output-s3/blob/master/CHANGELOG.md#406[Changelog]



==== Getting Help

For questions about the plugin, open a topic in the http://discuss.elastic.co[Discuss] forums. For bugs or feature requests, open an issue in https://github.com/elastic/logstash[Github].
For the list of Elastic supported plugins, please consult the https://www.elastic.co/support/matrix#show_logstash_plugins[Elastic Support Matrix].

==== Description

This plugin batches and uploads logstash events into Amazon Simple Storage Service (Amazon S3).

Requirements:
* Amazon S3 Bucket and S3 Access Permissions (Typically access_key_id and secret_access_key)
* S3 PutObject permission

S3 outputs create temporary files into the OS' temporary directory, you can specify where to save them using the `temporary_directory` option.

S3 output files have the following format

ls.s3.312bc026-2f5d-49bc-ae9f-5940cf4ad9a6.2013-04-18T10.00.tag_hello.part0.txt


|=======
| ls.s3 | indicate logstash plugin s3 |
| 312bc026-2f5d-49bc-ae9f-5940cf4ad9a6 | a new, random uuid per file. |
| 2013-04-18T10.00 | represents the time whenever you specify time_file. |
| tag_hello | this indicates the event's tag. |
| part0 | this means if you indicate size_file then it will generate more parts if you file.size > size_file. When a file is full it will be pushed to the bucket and then deleted from the temporary directory. If a file is empty, it is simply deleted.  Empty files will not be pushed |
|=======

Crash Recovery:
* This plugin will recover and upload temporary log files after crash/abnormal termination when using `restore` set to true







#### Usage:
This is an example of logstash config:
[source,ruby]
output {
   s3{
     access_key_id => "crazy_key"             (required)
     secret_access_key => "monkey_access_key" (required)
     region => "eu-west-1"                    (optional, default = "us-east-1")
     bucket => "your_bucket"                  (required)
     size_file => 2048                        (optional) - Bytes
     time_file => 5                           (optional) - Minutes
     format => "plain"                        (optional)
     canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
   }


&nbsp;

==== Synopsis

This plugin supports the following configuration options:

Required configuration options:

[source,json]
--------------------------
s3 {
    bucket => ...
}
--------------------------



Available configuration options:

[cols="<,<,<",options="header",]
|=======================================================================
|Setting |Input type|Required
| <<plugins-outputs-s3-access_key_id>> |<<string,string>>|No
| <<plugins-outputs-s3-aws_credentials_file>> |<<string,string>>|No
| <<plugins-outputs-s3-bucket>> |<<string,string>>|Yes
| <<plugins-outputs-s3-canned_acl>> |<<string,string>>, one of `["private", "public_read", "public_read_write", "authenticated_read"]`|No
| <<plugins-outputs-s3-codec>> |<<codec,codec>>|No
| <<plugins-outputs-s3-enable_metric>> |<<boolean,boolean>>|No
| <<plugins-outputs-s3-encoding>> |<<string,string>>, one of `["none", "gzip"]`|No
| <<plugins-outputs-s3-id>> |<<string,string>>|No
| <<plugins-outputs-s3-prefix>> |<<string,string>>|No
| <<plugins-outputs-s3-proxy_uri>> |<<string,string>>|No
| <<plugins-outputs-s3-region>> |<<string,string>>, one of `["us-east-1", "us-east-2", "us-west-1", "us-west-2", "eu-central-1", "eu-west-1", "eu-west-2", "ap-southeast-1", "ap-southeast-2", "ap-northeast-1", "ap-northeast-2", "sa-east-1", "us-gov-west-1", "cn-north-1", "ap-south-1", "ca-central-1"]`|No
| <<plugins-outputs-s3-restore>> |<<boolean,boolean>>|No
| <<plugins-outputs-s3-rotation_strategy>> |<<string,string>>, one of `["size_and_time", "size", "time"]`|No
| <<plugins-outputs-s3-secret_access_key>> |<<string,string>>|No
| <<plugins-outputs-s3-server_side_encryption>> |<<boolean,boolean>>|No
| <<plugins-outputs-s3-server_side_encryption_algorithm>> |<<string,string>>, one of `["AES256", "aws:kms"]`|No
| <<plugins-outputs-s3-session_token>> |<<string,string>>|No
| <<plugins-outputs-s3-signature_version>> |<<string,string>>, one of `["v2", "v4"]`|No
| <<plugins-outputs-s3-size_file>> |<<number,number>>|No
| <<plugins-outputs-s3-ssekms_key_id>> |<<string,string>>|No
| <<plugins-outputs-s3-storage_class>> |<<string,string>>, one of `["STANDARD", "REDUCED_REDUNDANCY", "STANDARD_IA"]`|No
| <<plugins-outputs-s3-tags>> |<<array,array>>|No
| <<plugins-outputs-s3-temporary_directory>> |<<string,string>>|No
| <<plugins-outputs-s3-time_file>> |<<number,number>>|No
| <<plugins-outputs-s3-upload_queue_size>> |<<number,number>>|No
| <<plugins-outputs-s3-upload_workers_count>> |<<number,number>>|No
| <<plugins-outputs-s3-validate_credentials_on_root_bucket>> |<<boolean,boolean>>|No
| <<plugins-outputs-s3-workers>> |<<,>>|No
|=======================================================================


==== Details

&nbsp;

[[plugins-outputs-s3-access_key_id]]
===== `access_key_id` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

This plugin uses the AWS SDK and supports several ways to get credentials, which will be tried in this order:

1. Static configuration, using `access_key_id` and `secret_access_key` params in logstash plugin config
2. External credentials file specified by `aws_credentials_file`
3. Environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
4. Environment variables `AMAZON_ACCESS_KEY_ID` and `AMAZON_SECRET_ACCESS_KEY`
5. IAM Instance Profile (available when running inside EC2)

[[plugins-outputs-s3-aws_credentials_file]]
===== `aws_credentials_file` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Path to YAML file containing a hash of AWS credentials.
This file will only be loaded if `access_key_id` and
`secret_access_key` aren't set. The contents of the
file should look like this:

[source,ruby]
----------------------------------
    :access_key_id: "12345"
    :secret_access_key: "54321"
----------------------------------


[[plugins-outputs-s3-bucket]]
===== `bucket` 

  * This is a required setting.
  * Value type is <<string,string>>
  * There is no default value for this setting.

S3 bucket

[[plugins-outputs-s3-canned_acl]]
===== `canned_acl` 

  * Value can be any of: `private`, `public_read`, `public_read_write`, `authenticated_read`
  * Default value is `"private"`

The S3 canned ACL to use when putting the file. Defaults to "private".

[[plugins-outputs-s3-codec]]
===== `codec` 

  * Value type is <<codec,codec>>
  * Default value is `"plain"`

The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.

[[plugins-outputs-s3-enable_metric]]
===== `enable_metric` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`

Disable or enable metric logging for this specific plugin instance
by default we record all the metrics we can, but you can disable metrics collection
for a specific plugin.

[[plugins-outputs-s3-encoding]]
===== `encoding` 

  * Value can be any of: `none`, `gzip`
  * Default value is `"none"`

Specify the content encoding. Supports ("gzip"). Defaults to "none"

[[plugins-outputs-s3-id]]
===== `id` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one. 
It is strongly recommended to set this ID in your configuration. This is particularly useful 
when you have two or more plugins of the same type, for example, if you have 2 grok filters. 
Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.

[source,ruby]
---------------------------------------------------------------------------------------------------
output {
 stdout {
   id => "my_plugin_id"
 }
}
---------------------------------------------------------------------------------------------------


[[plugins-outputs-s3-prefix]]
===== `prefix` 

  * Value type is <<string,string>>
  * Default value is `""`

Specify a prefix to the uploaded filename, this can simulate directories on S3.  Prefix does not require leading slash.
This option support string interpolation, be warned this can created a lot of temporary local files.

[[plugins-outputs-s3-proxy_uri]]
===== `proxy_uri` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

URI to proxy server if required

[[plugins-outputs-s3-region]]
===== `region` 

  * Value can be any of: `us-east-1`, `us-east-2`, `us-west-1`, `us-west-2`, `eu-central-1`, `eu-west-1`, `eu-west-2`, `ap-southeast-1`, `ap-southeast-2`, `ap-northeast-1`, `ap-northeast-2`, `sa-east-1`, `us-gov-west-1`, `cn-north-1`, `ap-south-1`, `ca-central-1`
  * Default value is `"us-east-1"`

The AWS Region

[[plugins-outputs-s3-restore]]
===== `restore` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`



[[plugins-outputs-s3-rotation_strategy]]
===== `rotation_strategy` 

  * Value can be any of: `size_and_time`, `size`, `time`
  * Default value is `"size_and_time"`

Define the strategy to use to decide when we need to rotate the file and push it to S3,
The default strategy is to check for both size and time, the first one to match will rotate the file.

[[plugins-outputs-s3-secret_access_key]]
===== `secret_access_key` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

The AWS Secret Access Key

[[plugins-outputs-s3-server_side_encryption]]
===== `server_side_encryption` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Specifies wether or not to use S3's server side encryption. Defaults to no encryption.

[[plugins-outputs-s3-server_side_encryption_algorithm]]
===== `server_side_encryption_algorithm` 

  * Value can be any of: `AES256`, `aws:kms`
  * Default value is `"AES256"`

Specifies what type of encryption to use when SSE is enabled.

[[plugins-outputs-s3-session_token]]
===== `session_token` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

The AWS Session token for temporary credential

[[plugins-outputs-s3-signature_version]]
===== `signature_version` 

  * Value can be any of: `v2`, `v4`
  * There is no default value for this setting.

The version of the S3 signature hash to use. Normally uses the internal client default, can be explicitly
specified here

[[plugins-outputs-s3-size_file]]
===== `size_file` 

  * Value type is <<number,number>>
  * Default value is `5242880`

Set the size of file in bytes, this means that files on bucket when have dimension > file_size, they are stored in two or more file.
If you have tags then it will generate a specific size file for every tags

[[plugins-outputs-s3-ssekms_key_id]]
===== `ssekms_key_id` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

The key to use when specified along with server_side_encryption => aws:kms.
If server_side_encryption => aws:kms is set but this is not default KMS key is used.
http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html

[[plugins-outputs-s3-storage_class]]
===== `storage_class` 

  * Value can be any of: `STANDARD`, `REDUCED_REDUNDANCY`, `STANDARD_IA`
  * Default value is `"STANDARD"`

Specifies what S3 storage class to use when uploading the file.
More information about the different storage classes can be found:
http://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html
Defaults to STANDARD.

[[plugins-outputs-s3-tags]]
===== `tags` 

  * Value type is <<array,array>>
  * Default value is `[]`

Define tags to be appended to the file on the S3 bucket.

Example:
tags => ["elasticsearch", "logstash", "kibana"]

Will generate this file:
"ls.s3.logstash.local.2015-01-01T00.00.tag_elasticsearch.logstash.kibana.part0.txt"


[[plugins-outputs-s3-temporary_directory]]
===== `temporary_directory` 

  * Value type is <<string,string>>
  * Default value is `"/var/folders/s9/fp9p03r525d_cgclm9ps2n6r0000gn/T/logstash"`

Set the directory where logstash will store the tmp files before sending it to S3
default to the current OS temporary directory in linux /tmp/logstash

[[plugins-outputs-s3-time_file]]
===== `time_file` 

  * Value type is <<number,number>>
  * Default value is `15`

Set the time, in MINUTES, to close the current sub_time_section of bucket.
If you define file_size you have a number of files in consideration of the section and the current tag.
0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
for now the only thing this plugin can do is to put the file when logstash restart.

[[plugins-outputs-s3-upload_queue_size]]
===== `upload_queue_size` 

  * Value type is <<number,number>>
  * Default value is `4`

Number of items we can keep in the local queue before uploading them

[[plugins-outputs-s3-upload_workers_count]]
===== `upload_workers_count` 

  * Value type is <<number,number>>
  * Default value is `4`

Specify how many workers to use to upload the files to S3

[[plugins-outputs-s3-validate_credentials_on_root_bucket]]
===== `validate_credentials_on_root_bucket` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`

The common use case is to define permission on the root bucket and give Logstash full access to write its logs.
In some circonstances you need finer grained permission on subfolder, this allow you to disable the check at startup.

[[plugins-outputs-s3-workers]]
===== `workers` 

  * Value type is <<string,string>>
  * Default value is `1`

