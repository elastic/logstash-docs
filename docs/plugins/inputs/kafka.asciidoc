[[plugins-inputs-kafka]]
=== kafka



This input will read events from a Kafka topic. It uses the high level consumer API provided
by Kafka to read messages from the broker. It also maintains the state of what has been
consumed using Zookeeper. The default input codec is json

You must configure `topic_id`, `white_list` or `black_list`. By default it will connect to a
Zookeeper running on localhost. All the broker information is read from Zookeeper state

Ideally you should have as many threads as the number of partitions for a perfect balance --
more threads than partitions means that some threads will be idle

For more information see http://kafka.apache.org/documentation.html#theconsumer

Kafka consumer configuration: http://kafka.apache.org/documentation.html#consumerconfigs


&nbsp;

==== Synopsis

This plugin supports the following configuration options:


Required configuration options:

[source,json]
--------------------------
kafka {
}
--------------------------



Available configuration options:

[cols="<,<,<,<m",options="header",]
|=======================================================================
|Setting |Input type|Required|Default value
| <<plugins-inputs-kafka-add_field>> |<<hash,hash>>|No|`{}`
| <<plugins-inputs-kafka-auto_offset_reset>> |<<string,string>>, one of `["largest", "smallest"]`|No|`"largest"`
| <<plugins-inputs-kafka-black_list>> |<<string,string>>|No|`nil`
| <<plugins-inputs-kafka-codec>> |<<codec,codec>>|No|`"json"`
| <<plugins-inputs-kafka-consumer_id>> |<<string,string>>|No|`nil`
| <<plugins-inputs-kafka-consumer_restart_on_error>> |<<boolean,boolean>>|No|`true`
| <<plugins-inputs-kafka-consumer_restart_sleep_ms>> |<<number,number>>|No|`0`
| <<plugins-inputs-kafka-consumer_threads>> |<<number,number>>|No|`1`
| <<plugins-inputs-kafka-consumer_timeout_ms>> |<<number,number>>|No|`-1`
| <<plugins-inputs-kafka-decoder_class>> |<<string,string>>|No|`"kafka.serializer.DefaultDecoder"`
| <<plugins-inputs-kafka-decorate_events>> |<<boolean,boolean>>|No|`false`
| <<plugins-inputs-kafka-fetch_message_max_bytes>> |<<number,number>>|No|`1048576`
| <<plugins-inputs-kafka-group_id>> |<<string,string>>|No|`"logstash"`
| <<plugins-inputs-kafka-key_decoder_class>> |<<string,string>>|No|`"kafka.serializer.DefaultDecoder"`
| <<plugins-inputs-kafka-queue_size>> |<<number,number>>|No|`20`
| <<plugins-inputs-kafka-rebalance_backoff_ms>> |<<number,number>>|No|`2000`
| <<plugins-inputs-kafka-rebalance_max_retries>> |<<number,number>>|No|`4`
| <<plugins-inputs-kafka-reset_beginning>> |<<boolean,boolean>>|No|`false`
| <<plugins-inputs-kafka-tags>> |<<array,array>>|No|
| <<plugins-inputs-kafka-topic_id>> |<<string,string>>|No|`nil`
| <<plugins-inputs-kafka-type>> |<<string,string>>|No|
| <<plugins-inputs-kafka-white_list>> |<<string,string>>|No|`nil`
| <<plugins-inputs-kafka-zk_connect>> |<<string,string>>|No|`"localhost:2181"`
|=======================================================================



==== Details

&nbsp;

[[plugins-inputs-kafka-add_field]]
===== `add_field` 

  * Value type is <<hash,hash>>
  * Default value is `{}`

Add a field to an event

[[plugins-inputs-kafka-auto_offset_reset]]
===== `auto_offset_reset` 

  * Value can be any of: `largest`, `smallest`
  * Default value is `"largest"`

`smallest` or `largest` - (optional, default `largest`) If the consumer does not already
have an established offset or offset is invalid, start with the earliest message present in the
log (`smallest`) or after the last message in the log (`largest`).

[[plugins-inputs-kafka-black_list]]
===== `black_list` 

  * Value type is <<string,string>>
  * Default value is `nil`

Blacklist of topics to exclude from consumption.

[[plugins-inputs-kafka-codec]]
===== `codec` 

  * Value type is <<codec,codec>>
  * Default value is `"json"`

The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.

[[plugins-inputs-kafka-consumer_id]]
===== `consumer_id` 

  * Value type is <<string,string>>
  * Default value is `nil`

A unique id for the consumer; generated automatically if not set.

[[plugins-inputs-kafka-consumer_restart_on_error]]
===== `consumer_restart_on_error` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`

Option to restart the consumer loop on error

[[plugins-inputs-kafka-consumer_restart_sleep_ms]]
===== `consumer_restart_sleep_ms` 

  * Value type is <<number,number>>
  * Default value is `0`

Time in millis to wait for consumer to restart after an error

[[plugins-inputs-kafka-consumer_threads]]
===== `consumer_threads` 

  * Value type is <<number,number>>
  * Default value is `1`

Number of threads to read from the partitions. Ideally you should have as many threads as the
number of partitions for a perfect balance. More threads than partitions means that some
threads will be idle. Less threads means a single thread could be consuming from more than
one partition

[[plugins-inputs-kafka-consumer_timeout_ms]]
===== `consumer_timeout_ms` 

  * Value type is <<number,number>>
  * Default value is `-1`

Throw a timeout exception to the consumer if no message is available for consumption after
the specified interval

[[plugins-inputs-kafka-decoder_class]]
===== `decoder_class` 

  * Value type is <<string,string>>
  * Default value is `"kafka.serializer.DefaultDecoder"`

The serializer class for messages. The default decoder takes a byte[] and returns the same byte[]

[[plugins-inputs-kafka-decorate_events]]
===== `decorate_events` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Option to add Kafka metadata like topic, message size to the event.
This will add a field named `kafka` to the logstash event containing the following attributes:
  `msg_size`: The complete serialized size of this message in bytes (including crc, header attributes, etc)
  `topic`: The topic this message is associated with
  `consumer_group`: The consumer group used to read in this event
  `partition`: The partition this message is associated with
  `key`: A ByteBuffer containing the message key

[[plugins-inputs-kafka-fetch_message_max_bytes]]
===== `fetch_message_max_bytes` 

  * Value type is <<number,number>>
  * Default value is `1048576`

The number of byes of messages to attempt to fetch for each topic-partition in each fetch
request. These bytes will be read into memory for each partition, so this helps control
the memory used by the consumer. The fetch request size must be at least as large as the
maximum message size the server allows or else it is possible for the producer to send
messages larger than the consumer can fetch.

[[plugins-inputs-kafka-group_id]]
===== `group_id` 

  * Value type is <<string,string>>
  * Default value is `"logstash"`

A string that uniquely identifies the group of consumer processes to which this consumer
belongs. By setting the same group id multiple processes indicate that they are all part of
the same consumer group.

[[plugins-inputs-kafka-key_decoder_class]]
===== `key_decoder_class` 

  * Value type is <<string,string>>
  * Default value is `"kafka.serializer.DefaultDecoder"`

The serializer class for keys (defaults to the same default as for messages)

[[plugins-inputs-kafka-queue_size]]
===== `queue_size` 

  * Value type is <<number,number>>
  * Default value is `20`

Internal Logstash queue size used to hold events in memory after it has been read from Kafka

[[plugins-inputs-kafka-rebalance_backoff_ms]]
===== `rebalance_backoff_ms` 

  * Value type is <<number,number>>
  * Default value is `2000`

Backoff time between retries during rebalance.

[[plugins-inputs-kafka-rebalance_max_retries]]
===== `rebalance_max_retries` 

  * Value type is <<number,number>>
  * Default value is `4`

When a new consumer joins a consumer group the set of consumers attempt to "rebalance" the
load to assign partitions to each consumer. If the set of consumers changes while this
assignment is taking place the rebalance will fail and retry. This setting controls the
maximum number of attempts before giving up.

[[plugins-inputs-kafka-reset_beginning]]
===== `reset_beginning` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Reset the consumer group to start at the earliest message present in the log by clearing any
offsets for the group stored in Zookeeper. This is destructive! Must be used in conjunction
with auto_offset_reset => 'smallest'

[[plugins-inputs-kafka-tags]]
===== `tags` 

  * Value type is <<array,array>>
  * There is no default value for this setting.

Add any number of arbitrary tags to your event.

This can help with processing later.

[[plugins-inputs-kafka-topic_id]]
===== `topic_id` 

  * Value type is <<string,string>>
  * Default value is `nil`

The topic to consume messages from

[[plugins-inputs-kafka-type]]
===== `type` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Add a `type` field to all events handled by this input.

Types are used mainly for filter activation.

The type is stored as part of the event itself, so you can
also use the type to search for it in Kibana.

If you try to set a type on an event that already has one (for
example when you send an event from a shipper to an indexer) then
a new input will not override the existing type. A type set at
the shipper stays with that event for its life even
when sent to another Logstash server.

[[plugins-inputs-kafka-white_list]]
===== `white_list` 

  * Value type is <<string,string>>
  * Default value is `nil`

Whitelist of topics to include for consumption.

[[plugins-inputs-kafka-zk_connect]]
===== `zk_connect` 

  * Value type is <<string,string>>
  * Default value is `"localhost:2181"`

Specifies the ZooKeeper connection string in the form hostname:port where host and port are
the host and port of a ZooKeeper server. You can also specify multiple hosts in the form
`hostname1:port1,hostname2:port2,hostname3:port3`.

The server may also have a ZooKeeper chroot path as part of it's ZooKeeper connection string
which puts its data under some path in the global ZooKeeper namespace. If so the consumer
should use the same chroot path in its connection string. For example to give a chroot path of
`/chroot/path` you would give the connection string as
`hostname1:port1,hostname2:port2,hostname3:port3/chroot/path`.


